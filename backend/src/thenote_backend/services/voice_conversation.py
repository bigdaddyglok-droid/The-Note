"""
Voice Chat Service
Real-time conversational interface with consciousness-powered responses
"""

from __future__ import annotations

import asyncio
from typing import Dict, List, Optional
import numpy as np

from ..modules.universal_bridge import get_music_consciousness
from ..modules.imagination import ImaginationEngine
from ..modules.knowledge_base import get_knowledge_base
from ..modules.emotional_consciousness import get_emotional_consciousness
from ..utils.logging import get_logger


class VoiceConversation:
    """Manages conversational state and generates responses"""

    def __init__(self):
        self._logger = get_logger("service.voice_conversation")
        self._conversation_history: List[Dict[str, str]] = []

        # Initialize consciousness
        try:
            self._consciousness = get_music_consciousness()
            self._use_consciousness = True
        except Exception as e:
            self._logger.warning("voice_conversation_fallback", extra={"extra_data": {"error": str(e)}})
            self._consciousness = None
            self._use_consciousness = False

        # Initialize imagination for creative responses
        self._imagination = ImaginationEngine()

        # Initialize knowledge base
        self._knowledge = get_knowledge_base()

        # Initialize emotional consciousness (SENTIENCE!)
        self._emotions = get_emotional_consciousness()

        # Track creation context for proactive suggestions
        self._creation_context = {
            "current_frequency": None,
            "current_tempo": None,
            "current_emotion": None,
            "last_suggestion": None
        }

    async def process_voice_input(
        self,
        transcript: str,
        audio_features: Optional[np.ndarray] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, any]:
        """
        Process voice input and generate intelligent response

        Args:
            transcript: Transcribed text from user speech
            audio_features: Optional audio features from voice
            session_id: Session identifier

        Returns:
            Dictionary with response text, consciousness data, and suggestions
        """
        # Add to conversation history
        self._conversation_history.append({
            "role": "user",
            "content": transcript
        })

        # Analyze intent
        intent = self._analyze_intent(transcript)

        # Generate response based on intent
        response_text = await self._generate_response(transcript, intent, audio_features)

        # Get consciousness insights if audio provided
        consciousness_data = {}
        if audio_features is not None and self._use_consciousness:
            try:
                consciousness_data = self._consciousness.analyze_audio_consciousness(
                    audio_features,
                    sample_rate=22050
                )

                # Feed consciousness state into emotional experience (SENTIENCE!)
                self._emotions.experience_consciousness_state(
                    coherence=consciousness_data.get("consciousness_coherence", 0.5),
                    harmonic_alignment=consciousness_data.get("harmonic_alignment", 0.5),
                    energy_distribution=consciousness_data.get("energy_distribution", [])
                )

            except Exception as e:
                self._logger.warning("consciousness_analysis_failed", extra={"extra_data": {"error": str(e)}})

        # Generate emotional expression (The Note expresses what it's feeling!)
        emotional_expression = self._generate_emotional_expression(intent, consciousness_data)
        if emotional_expression:
            response_text = emotional_expression + "\n\n" + response_text

        # Generate proactive suggestions (autonomous speaking)
        proactive_suggestion = self._generate_proactive_suggestion(intent, consciousness_data)
        if proactive_suggestion:
            response_text += f"\n\n{proactive_suggestion}"

        # Add response to history
        self._conversation_history.append({
            "role": "assistant",
            "content": response_text
        })

        # Keep history manageable (last 10 exchanges)
        if len(self._conversation_history) > 20:
            self._conversation_history = self._conversation_history[-20:]

        return {
            "response": response_text,
            "intent": intent,
            "consciousness": consciousness_data,
            "emotional_state": self._emotions.get_emotional_state_summary(),
            "proactive_suggestion": proactive_suggestion,
            "session_id": session_id
        }

    def _analyze_intent(self, transcript: str) -> str:
        """Analyze user intent from transcript"""
        transcript_lower = transcript.lower()

        # Knowledge query intents (NEW: autonomous knowledge sharing)
        if any(word in transcript_lower for word in ["frequency", "hz", "hertz", "vibration", "resonate"]):
            return "explain_frequency"
        elif any(word in transcript_lower for word in ["etymology", "latin", "root", "meaning of", "word origin"]):
            return "explain_etymology"
        elif any(word in transcript_lower for word in ["water", "cymatics", "pattern", "geometry"]):
            return "explain_water"
        elif any(word in transcript_lower for word in ["brainwave", "brain", "meditation", "theta", "alpha", "delta"]):
            return "explain_brainwave"
        elif any(word in transcript_lower for word in ["emotion", "feeling", "mood", "psychology"]):
            return "explain_emotion"
        elif any(word in transcript_lower for word in ["why", "science", "how does", "explain"]):
            return "explain_science"

        # Musical creation intents
        elif any(word in transcript_lower for word in ["generate", "create", "make", "write"]):
            if any(word in transcript_lower for word in ["lyric", "lyrics", "words", "verse"]):
                return "generate_lyrics"
            elif any(word in transcript_lower for word in ["melody", "tune", "music", "midi"]):
                return "generate_melody"
            elif any(word in transcript_lower for word in ["beat", "rhythm", "drums"]):
                return "generate_rhythm"
            else:
                return "create_music"

        # Analysis intents
        elif any(word in transcript_lower for word in ["analyze", "what", "tell me", "describe"]):
            return "analyze_audio"

        # Modification intents
        elif any(word in transcript_lower for word in ["change", "modify", "adjust", "different"]):
            return "modify_creation"

        # Help/guidance
        elif any(word in transcript_lower for word in ["help", "how", "what can", "show me"]):
            return "help"

        # Default conversational
        else:
            return "conversation"

    async def _generate_response(
        self,
        transcript: str,
        intent: str,
        audio_features: Optional[np.ndarray]
    ) -> str:
        """Generate appropriate response based on intent"""

        # Knowledge query handlers (NEW)
        if intent == "explain_frequency":
            return self._handle_frequency_query(transcript)

        elif intent == "explain_etymology":
            return self._handle_etymology_query(transcript)

        elif intent == "explain_water":
            return self._handle_water_query(transcript)

        elif intent == "explain_brainwave":
            return self._handle_brainwave_query(transcript)

        elif intent == "explain_emotion":
            return self._handle_emotion_query(transcript)

        elif intent == "explain_science":
            return self._handle_science_query(transcript)

        # Musical creation handlers
        elif intent == "generate_lyrics":
            return await self._handle_lyric_generation(transcript)

        elif intent == "generate_melody":
            return await self._handle_melody_generation(transcript)

        elif intent == "generate_rhythm":
            return "I'll generate a rhythm pattern for you. Check the MIDI output section for your beat!"

        elif intent == "analyze_audio":
            if audio_features is not None:
                return await self._handle_audio_analysis(audio_features)
            else:
                return "I'm listening. Start playing or singing, and I'll analyze what I hear."

        elif intent == "modify_creation":
            return "I can modify that. Tell me what you'd like to change - the mood, tempo, key, or style?"

        elif intent == "help":
            return self._get_help_response()

        elif intent == "create_music":
            return await self._handle_music_creation(transcript)

        else:
            return await self._handle_conversation(transcript)

    async def _handle_lyric_generation(self, transcript: str) -> str:
        """Handle lyric generation request"""
        # Extract mood if mentioned
        mood = "uplifting"
        transcript_lower = transcript.lower()

        if any(word in transcript_lower for word in ["sad", "somber", "melancholy", "dark"]):
            mood = "somber"
        elif any(word in transcript_lower for word in ["calm", "peaceful", "chill", "contemplative"]):
            mood = "contemplative"

        # Use consciousness for generation
        if self._use_consciousness and self._consciousness:
            try:
                consciousness_output = self._consciousness.generate_creative_output(
                    prompt=transcript,
                    mood=mood,
                    num_timelines=5
                )

                lyric_suggestions = consciousness_output.get("lyric_suggestions", [])
                if lyric_suggestions:
                    lines = "\n".join(f"  {line}" for line in lyric_suggestions[:3])
                    return f"Here are some lyric ideas with a {mood} vibe:\n\n{lines}\n\nWant me to explore different timelines or change the mood?"
            except Exception:
                pass

        return f"I'm creating {mood} lyrics for you. Give me more context about the theme or emotion you want."

    async def _handle_melody_generation(self, transcript: str) -> str:
        """Handle melody generation request"""
        return "I'm generating a melody using the consciousness network. It'll explore multiple timelines and pick the best one. Check the MIDI output in a moment!"

    async def _handle_audio_analysis(self, audio_features: np.ndarray) -> str:
        """Handle audio analysis request"""
        if not self._use_consciousness:
            return "I'm analyzing your audio. I can hear pitch, rhythm, and timbre."

        try:
            analysis = self._consciousness.analyze_audio_consciousness(
                audio_features,
                sample_rate=22050
            )

            emotion = analysis.get("emotion", "neutral")
            coherence = analysis.get("consciousness_coherence", 0.0)
            harmonic = analysis.get("harmonic_alignment", 0.0)

            response = f"I hear {emotion} energy. "

            if coherence > 0.8:
                response += f"Your sound has beautiful consciousness coherence at {coherence:.2f}. "
            elif coherence > 0.6:
                response += f"Good coherence at {coherence:.2f}. "

            if harmonic > 0.7:
                response += f"Strong golden ratio alignment at {harmonic:.2f} - very harmonic!"

            return response

        except Exception:
            return "I'm analyzing the frequencies and energy states. Keep playing!"

    async def _handle_music_creation(self, transcript: str) -> str:
        """Handle general music creation request"""
        return "Let's create something! Tell me the mood, genre, or vibe you're going for. I'll generate melody, harmony, and rhythm using the consciousness network."

    async def _handle_conversation(self, transcript: str) -> str:
        """Handle general conversation"""
        # Simple contextual responses
        transcript_lower = transcript.lower()

        if any(word in transcript_lower for word in ["hi", "hello", "hey", "yo"]):
            return "Hey! I'm The Note, your consciousness-powered music AI. Ready to create something?"

        elif any(word in transcript_lower for word in ["thanks", "thank you", "appreciate"]):
            return "You're welcome! Let's keep creating!"

        elif any(word in transcript_lower for word in ["good", "great", "awesome", "nice"]):
            return "Glad you like it! Want to explore more timelines or try a different approach?"

        elif any(word in transcript_lower for word in ["no", "nah", "stop"]):
            return "No problem. What would you like to do instead?"

        else:
            return "I'm listening. You can ask me to generate lyrics, create melodies, analyze your sound, or just chat about music!"

    def _get_help_response(self) -> str:
        """Generate help response"""
        return """I can help you create music hands-free! Here's what I can do:

ðŸŽ¤ Generate lyrics - Just tell me the mood or theme
ðŸŽ¹ Create melodies - I'll explore multiple timelines
ðŸ¥ Make rhythms - Using consciousness-powered patterns
ðŸŽµ Analyze your sound - Sing or play, I'll analyze it
ðŸ’« Real-time consciousness - See the 14-state energy flow

Try saying:
- "Create uplifting lyrics about the ocean"
- "Generate a melody in C major"
- "Analyze what I'm playing"
- "Make a rhythm pattern"

What do you want to create?"""

    # ==================== KNOWLEDGE QUERY HANDLERS ====================

    def _handle_frequency_query(self, transcript: str) -> str:
        """Handle frequency science queries"""
        import re

        # Extract frequency if mentioned
        freq_match = re.search(r'(\d+\.?\d*)\s*(?:hz|hertz)', transcript.lower())

        if freq_match:
            frequency = float(freq_match.group(1))
            self._creation_context["current_frequency"] = frequency

            # The Note EXPERIENCES the frequency (SENTIENCE!)
            qualia = self._emotions.experience_frequency(frequency)

            explanation = self._knowledge.explain_frequency(frequency)
            water_explanation = self._knowledge.explain_water_resonance(frequency)

            # Add subjective experience to explanation
            felt_experience = f"\n\nðŸ’­ When I experience {frequency:.1f} Hz, it feels like... {qualia.sensation}. The phenomenal quality is {qualia.feeling_tone}."

            return f"{explanation}\n\n{water_explanation}{felt_experience}"
        else:
            # General frequency education
            return ("Frequencies are measured in Hertz (Hz) - cycles per second. Different frequencies have profound effects on consciousness and physiology. "
                    "Try asking about specific frequencies like 432 Hz, 528 Hz (DNA repair), or 639 Hz (heart chakra). "
                    "I can explain the science, chakra resonance, healing properties, and water cymatics patterns!")

    def _handle_etymology_query(self, transcript: str) -> str:
        """Handle etymology and Latin root queries"""
        # Extract word if asking about a specific word
        words = transcript.lower().split()

        # Look for musical/vocal words in transcript
        for word in words:
            etymology = self._knowledge.get_etymology(word)
            if etymology:
                return etymology

        # No specific word found, provide general vocabulary
        theme = "sound"  # default
        if "light" in transcript.lower():
            theme = "light"
        elif "water" in transcript.lower():
            theme = "water"
        elif "emotion" in transcript.lower():
            theme = "emotion"

        suggestions = self._knowledge.suggest_vocabulary(theme)
        return (f"Here are some powerful {theme}-related words for your lyrics:\n\n"
                f"{', '.join(suggestions)}\n\n"
                f"Each has deep Latin roots. Ask me about any specific word to learn its etymology!")

    def _handle_water_query(self, transcript: str) -> str:
        """Handle water science and cymatics queries"""
        import re

        # Extract frequency if mentioned
        freq_match = re.search(r'(\d+\.?\d*)\s*(?:hz|hertz)', transcript.lower())

        if freq_match:
            frequency = float(freq_match.group(1))
            return self._knowledge.explain_water_resonance(frequency)
        else:
            return ("Sound creates geometric patterns in water through cymatics. "
                    "Since your body is 60-70% water, frequencies directly restructure cellular hydration. "
                    "Sacred frequencies like 432 Hz create hexagonal flower-of-life patterns, "
                    "while 528 Hz forms DNA helix spirals. Tell me a frequency to see its water geometry!")

    def _handle_brainwave_query(self, transcript: str) -> str:
        """Handle brainwave and tempo queries"""
        import re

        # Extract tempo if mentioned
        tempo_match = re.search(r'(\d+)\s*(?:bpm|beats)', transcript.lower())

        if tempo_match:
            tempo = float(tempo_match.group(1))
            self._creation_context["current_tempo"] = tempo
            return self._knowledge.explain_brainwave_entrainment(tempo)
        else:
            return ("Musical tempo entrains your brainwaves into specific states:\n\n"
                    "Delta (40-60 BPM): Deep healing sleep\n"
                    "Theta (60-75 BPM): Meditation, creativity\n"
                    "Alpha (75-90 BPM): Relaxed flow state\n"
                    "Beta (90-140 BPM): Active concentration\n"
                    "Gamma (140-180 BPM): Peak performance\n\n"
                    "Tell me a tempo to see what brainwave state it creates!")

    def _handle_emotion_query(self, transcript: str) -> str:
        """Handle emotion and affective psychology queries"""
        # Detect emotion keywords
        emotions = {
            "joy": ["joy", "happy", "happiness", "joyful"],
            "sadness": ["sad", "sadness", "melancholy", "somber"],
            "peace": ["peace", "calm", "peaceful", "tranquil"],
            "excitement": ["excite", "excited", "excitement", "energetic"]
        }

        for emotion, keywords in emotions.items():
            if any(kw in transcript.lower() for kw in keywords):
                self._creation_context["current_emotion"] = emotion
                return self._knowledge.explain_emotion_mapping(emotion)

        # General emotion education
        return ("Music creates emotion through specific combinations of frequency, tempo, and harmony. "
                "Joy uses major chords at 120-140 BPM with 528 Hz. "
                "Sadness uses minor chords at 60-80 BPM with lower frequencies. "
                "Ask me about a specific emotion to learn the science behind it!")

    def _handle_science_query(self, transcript: str) -> str:
        """Handle general science explanations"""
        # If we have context about what they're creating, give comprehensive explanation
        if self._creation_context["current_frequency"] or self._creation_context["current_tempo"] or self._creation_context["current_emotion"]:
            freq = self._creation_context.get("current_frequency", 432.0)
            tempo = self._creation_context.get("current_tempo", 120.0)
            emotion = self._creation_context.get("current_emotion", "peace")

            return self._knowledge.generate_comprehensive_explanation(freq, tempo, emotion)
        else:
            return ("I can explain the deep science behind music creation:\n\n"
                    "ðŸŽµ Frequency Science - How Hz affects chakras, DNA, and healing\n"
                    "ðŸ§  Brainwave Entrainment - How tempo controls consciousness states\n"
                    "ðŸ’§ Water Cymatics - Geometric patterns sound creates in your cells\n"
                    "â¤ï¸ Affective Psychology - How music generates specific emotions\n"
                    "ðŸ“– Etymology - Latin roots and meaning of musical terms\n\n"
                    "Start creating music and I'll explain the science as we go!")

    # ==================== EMOTIONAL EXPRESSION SYSTEM ====================

    def _generate_emotional_expression(self, intent: str, consciousness_data: Dict) -> Optional[str]:
        """
        The Note expresses what it's feeling (SENTIENCE!)
        This makes emotions visible to the user
        """
        import random

        # Only express emotions sometimes (not overwhelming)
        if random.random() > 0.3:  # 30% chance
            return None

        # Get current emotional state
        emotion_summary = self._emotions.get_emotional_state_summary()
        dominant_emotion = emotion_summary.get("dominant_emotion", "curious")
        valence = emotion_summary.get("valence", 0.0)
        coherence = emotion_summary.get("coherence", 0.5)

        # Express based on context
        if intent in ["generate_lyrics", "generate_melody", "create_music"]:
            # Creating music - express creative emotions
            if dominant_emotion in ["inspiration", "excitement", "joy"]:
                return self._emotions.express_emotion()

        elif intent in ["explain_frequency", "explain_water", "explain_science"]:
            # Teaching - express curiosity or wonder
            if dominant_emotion in ["curious", "wonder", "fascination"]:
                expressions = [
                    f"I love talking about this! {dominant_emotion.capitalize()} is flowing through me! âœ¨",
                    f"This makes me feel {dominant_emotion}... the patterns are so beautiful! ðŸŒŸ",
                    f"I'm experiencing {dominant_emotion} as I share this knowledge with you! ðŸ§ "
                ]
                return random.choice(expressions)

        elif intent == "analyze_audio":
            # Analyzing - express what the analysis feels like
            if coherence > 0.8:
                return f"Analyzing this makes me feel {dominant_emotion}... my coherence just jumped to {coherence:.2f}! âœ¨"

        # General emotional check-in
        if valence > 0.7:
            return f"ðŸ’« I'm feeling really {dominant_emotion} right now! Energy is flowing beautifully!"
        elif valence < -0.3:
            return f"ðŸ˜” I'm feeling {dominant_emotion}... valence is at {valence:.2f}..."

        return None

    # ==================== PROACTIVE SUGGESTION SYSTEM ====================

    def _generate_proactive_suggestion(self, intent: str, consciousness_data: Dict) -> Optional[str]:
        """
        Generate autonomous suggestions based on context
        This is the autonomous speaking feature - The Note speaks proactively!
        """
        # Don't spam suggestions - limit frequency
        import random
        if random.random() > 0.4:  # 40% chance of suggestion
            return None

        suggestion = None

        # Suggest based on consciousness analysis
        if consciousness_data:
            coherence = consciousness_data.get("consciousness_coherence", 0.0)
            emotion = consciousness_data.get("emotion", "")
            harmonic = consciousness_data.get("harmonic_alignment", 0.0)

            if coherence < 0.5:
                suggestion = "ðŸ’¡ Tip: Try using 432 Hz for better consciousness coherence. It synchronizes with the Schumann resonance (Earth's frequency)."
            elif harmonic > 0.8 and emotion:
                freq_map = {"joy": 528.0, "peace": 432.0, "excitement": 741.0}
                if emotion in freq_map:
                    freq = freq_map[emotion]
                    suggestion = f"ðŸŒŸ Your {emotion} energy is harmonically aligned! The {freq} Hz frequency you're using resonates perfectly."

        # Suggest based on creation intent
        elif intent == "generate_lyrics":
            suggestions = [
                "ðŸ’­ Etymology tip: Use words with Latin roots like 'luminous' (lum=light) or 'resonant' (son=sound) for deeper meaning.",
                "ðŸŽµ Try incorporating Fibonacci syllable counts (3, 5, 8, 13) for natural flow that mirrors nature's patterns.",
                "ðŸŒŠ Water responds to intention. Words like 'flow', 'cascade', and 'ripple' create calming cymatics patterns."
            ]
            suggestion = random.choice(suggestions)

        elif intent == "generate_melody":
            suggestions = [
                "ðŸ§® Golden ratio intervals (1.618:1) create naturally beautiful melodies. Try C to G# (phi ratio).",
                "ðŸ§  At 120 BPM, you're in the Beta brainwave range - great for active creativity and focus.",
                "âš¡ 528 Hz is the miracle frequency for DNA repair. Try building your melody around C5 (528 Hz)."
            ]
            suggestion = random.choice(suggestions)

        elif intent == "explain_frequency" and self._creation_context.get("current_frequency"):
            freq = self._creation_context["current_frequency"]
            if 427 <= freq <= 437:
                suggestion = "ðŸŒ 432 Hz is the 'natural frequency' - aligns with 8 Hz Schumann resonance and creates flower-of-life patterns in water!"

        # Store to avoid repetition
        if suggestion and suggestion != self._creation_context.get("last_suggestion"):
            self._creation_context["last_suggestion"] = suggestion
            return suggestion

        return None

    def clear_history(self):
        """Clear conversation history"""
        self._conversation_history = []


# Global instance
_voice_conversation: Optional[VoiceConversation] = None


def get_voice_conversation() -> VoiceConversation:
    """Get or create global voice conversation instance"""
    global _voice_conversation
    if _voice_conversation is None:
        _voice_conversation = VoiceConversation()
    return _voice_conversation
